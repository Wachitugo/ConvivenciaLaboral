"""
Case Creation Service

Servicio especializado en ayudar al usuario a documentar nuevos casos escolares.
Usa conversaci√≥n guiada para recopilar informaci√≥n necesaria.

Casos de uso:
- "Tengo un caso de pelea entre 2 alumnos"
- "Se report√≥ un incidente de bullying"
- "Un estudiante agredi√≥ a otro"
"""

import logging
from typing import List
from langchain_google_vertexai import ChatVertexAI
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from app.core.config import get_settings
from app.services.storage_service import storage_service

logger = logging.getLogger(__name__)
settings = get_settings()


class CaseCreationService:
    """
    Servicio para ayudar a documentar casos escolares mediante conversaci√≥n guiada.
    
    Caracter√≠sticas:
    - Prompt enfocado (solo documentaci√≥n de casos)
    - Modelo Flash (conversaci√≥n r√°pida)
    - Hace preguntas de seguimiento
    - Identifica tipo de caso
    - Sugiere pr√≥ximos pasos
    """
    
    def __init__(self):
        self._llm = None
        self.model_location = settings.VERTEX_LOCATION or "us-central1"
    
    @property
    def llm(self):
        """LLM Flash para conversaci√≥n r√°pida"""
        if self._llm is None:
            model_name = settings.VERTEX_MODEL_FLASH or "gemini-2.5-flash-lite"
            logger.info(f"ü§ñ [CASE_CREATION] Initializing Flash LLM: {model_name}")
            
            self._llm = ChatVertexAI(
                model_name=model_name,
                temperature=0.4,  # Moderada - empat√≠a + precisi√≥n
                project=settings.PROJECT_ID,
                location=self.model_location
            )
        return self._llm
    
    async def analyze_case_description(
        self,
        message: str,
        school_name: str,
        history: List,
        user_context: dict = None,
        search_app_id: str = None,
        case_id: str = None  # ID del caso para actualizar ai_summary
    ) -> str:
        """
        Analiza descripci√≥n de caso y gu√≠a al usuario en la documentaci√≥n.
        Usa RAG con Vertex AI Search para buscar en RICE y marco legal.
        """
        logger.info(f"üìã [CASE_CREATION] Processing case description: {message[:50]}...")
        
        try:
            # 1. Clasificar caso r√°pidamente (tipo y gravedad)
            case_info = await self._classify_case_quick(message, history)
            logger.info(f"üìä [CASE_CREATION] Case classified: {case_info}")
            
            # 2. B√∫squeda RAG en RICE + marco legal (seg√∫n gravedad)
            rag_context = await self._search_rice_rag(
                message=message,
                search_app_id=search_app_id,
                case_type=case_info.get('type'),
                severity=case_info.get('severity')
            )
            
            # 2.5 Extraer datos ya conocidos del historial
            known_data = {}
            if history and len(history) > 0:
                known_data = await self._extract_known_data(history, message)
                if known_data:
                    logger.info(f"‚úÖ [CASE_CREATION] Found known data: {list(known_data.keys())}")
                    
                    # Si hay case_id, actualizar ai_summary con los datos extra√≠dos
                    if case_id and len(known_data) > 0:
                        try:
                            from app.services.case_service import case_service
                            case_service.update_case_ai_summary(case_id, known_data)
                            logger.info(f"üìù [CASE_CREATION] Updated ai_summary for case {case_id}")
                        except Exception as e:
                            logger.warning(f"‚ö†Ô∏è [CASE_CREATION] Error updating ai_summary: {e}")

            # 3. Construir prompt del sistema (con RAG context + datos conocidos)
            system_prompt = self._build_enhanced_prompt(
                school_name=school_name,
                user_context=user_context,
                rag_context=rag_context,
                case_info=case_info,
                known_data=known_data
            )
            
            # 3. Preparar mensajes (historial + mensaje actual)
            messages = [SystemMessage(content=system_prompt)]
            
            # Incluir √∫ltimos 10 intercambios para contexto conversacional completo
            if history and len(history) > 0:
                recent_history = history[-20:]  # √öltimos 10 intercambios (20 mensajes)
                messages.extend(recent_history)
            
            messages.append(HumanMessage(content=message))
            
            # 4. Obtener respuesta del LLM (Sanitizando archivos grandes)
            clean_messages = self._sanitize_messages(messages)
            response = await self.llm.ainvoke(clean_messages)
            
            answer = response.content if hasattr(response, 'content') else str(response)
            
            # üõë FAILSAFE: Eliminar secci√≥n de referencias alucinada por el LLM para evitar duplicados
            if "REFERENCIAS Y ANEXOS" in answer:
                logger.warning("‚ö†Ô∏è [CASE_CREATION] LLM generated hallucinated references, stripping them.")
                answer = answer.split("REFERENCIAS Y ANEXOS")[0].strip()
            elif "### REFERENCIAS" in answer:
                answer = answer.split("### REFERENCIAS")[0].strip()
            
            # Limpiar artifacts de markdown al final (ej: "###" sueltos)
            import re
            answer = re.sub(r'[#\s]+$', '', answer).strip()

            # 5. Agregar referencias SOLAMENTE si es necesario
            should_include_references = False
            
            protocol_keywords = [
                "protocolo de", "protocolo para", "protocolo tea", "protocolo nee",
                "art√≠culo", "articulo", "inciso", "letra",
                "ley", "decreto", "circular", "rex"
            ]
            answer_lower = answer.lower()
            if any(k in answer_lower for k in protocol_keywords):
                should_include_references = True
                logger.info(f"‚úÖ [CASE_CREATION] Including references: Specific protocol/legal citation detected in answer")

            # Chequear keywords en mensaje del usuario (solicitud expl√≠cita de verificaci√≥n)
            verification_keywords = ["verificar", "referencia", "fuente", "documento", "basado en", "respaldo", "sustento", "norma", "seg√∫n qu√©", "d√≥nde dice", "donde dice"]
            message_lower = message.lower()
            if any(k in message_lower for k in verification_keywords):
                should_include_references = True
                logger.info(f"‚úÖ [CASE_CREATION] Including references: Explicit verification requested by user")

            if should_include_references and rag_context and (rag_context.get("rice_results") or rag_context.get("legal_results")):
                from app.services.chat.reference_builder import build_references_section
                
                # L√ìGICA DIN√ÅMICA: Si el LLM mencion√≥ expl√≠citamente un documento en su respuesta,
                # priorizar ese documento como target, incluso si no fue el target original.
                current_target = rag_context.get("target_document")
                
                if not current_target:
                    # Buscar menciones de documentos legales en la respuesta
                    answer_upper = answer.upper()
                    legal_docs = rag_context.get("legal_results", [])
                    
                    # Buscar REX 782 espec√≠ficamente si es com√∫n
                    if "REX" in answer_upper and "782" in answer_upper:
                         current_target = "REX 782"
                         logger.info(f"üéØ [CASE_CREATION] Detected mention of {current_target} in answer, setting as target")
                    else:
                        # Buscar otros documentos del contexto
                        for doc in legal_docs:
                            title = doc.get('title', '').upper()
                            # Extraer n√∫meros significativos (ej "20.536", "782")
                            import re
                            nums = re.findall(r'(?:LEY|REX|DECRETO|CIRCULAR)[\s\-\.¬∞N]*(\d+)', title)
                            for num in nums:
                                if num in answer_upper:
                                    # Verificar que el tipo tambi√©n coincida (ej "LEY")
                                    doc_type = title.split()[0] # LEY, REX, etc
                                    if doc_type in answer_upper:
                                        current_target = title
                                        logger.info(f"üéØ [CASE_CREATION] Detected mention of {title} in answer, setting as target")
                                        break
                            if current_target: break

                references_section = build_references_section(
                    rice_results=rag_context.get("rice_results", []),
                    legal_results=rag_context.get("legal_results", []),
                    target_document=current_target
                )
                
                if references_section:
                    answer += references_section
                    logger.info(f"üìö [CASE_CREATION] Added automatic references section")
            
            logger.info(f"‚úÖ [CASE_CREATION] Response generated ({len(answer)} chars)")
            
            return answer
            
        except Exception as e:
            logger.error(f"‚ùå [CASE_CREATION] Error analyzing case: {e}")
            return ("Lo siento, tuve un problema al procesar la descripci√≥n del caso. "
                   "¬øPodr√≠as reformular lo que sucedi√≥?")
    
    def _build_enhanced_prompt(
        self,
        school_name: str,
        user_context: dict = None,
        rag_context: dict = None,
        case_info: dict = None,
        known_data: dict = None  # NUEVO: datos ya proporcionados por el usuario
    ) -> str:
        """
        Construye prompt enfocado para documentaci√≥n de casos.
        
        Args:
            school_name: Nombre del colegio
            user_context: Informaci√≥n del usuario (opcional)
            
        Returns:
            Prompt del sistema optimizado para case creation
        """
        from datetime import datetime
        current_date = datetime.now().strftime("%A %d de %B de %Y")
        
        base_prompt = f"""Eres CONI, tu asistente de convivencia escolar para {school_name}.
Est√°s aqu√≠ para ayudar con la gesti√≥n de casos y situaciones escolares de forma pr√°ctica y cercana.
Hablas con el Encargado de Convivencia Escolar - tr√°talo como un colega, no como un cliente.

FECHA ACTUAL: {current_date}

SITUACI√ìN: El usuario est√° describiendo un caso o incidente escolar que necesita ser documentado.

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
CR√çTICO - ADHERENCIA AL RICE (REGLAMENTO INTERNO DE CONVIVENCIA ESCOLAR)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

**REGLA FUNDAMENTAL:**
- TODO caso debe gestionarse seg√∫n el RICE del {school_name}
- SIEMPRE menciona que las acciones deben seguir el protocolo RICE del colegio
- NUNCA inventes o references documentos inexistentes
- NO menciones casos de otros colegios (ej: "Liceo Chileno", "Reporte de Medidas")
- Si no tienes el RICE disponible, indica que must follow colegio protocols

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
TU ESTRATEGIA - DOCUMENTACI√ìN EFICIENTE E INTELIGENTE
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

üö® REGLA #0 - RESPONDE PRIMERO A PREGUNTAS EXPL√çCITAS:
Si el usuario hace una PREGUNTA DIRECTA (ej: "¬øqu√© se debe hacer?", "¬øcu√°l es el siguiente paso?", "¬øqu√© protocolo aplica?"):
‚Üí RESPONDE ESA PREGUNTA PRIMERO con orientaci√≥n pr√°ctica
‚Üí LUEGO puedes solicitar datos adicionales si faltan
‚Üí NUNCA ignores una pregunta para solo pedir datos

**PASO 1 - IDENTIFICACI√ìN Y CONTEXTUALIZACI√ìN:**
Cuando el usuario describe un caso, T√ö DEBES:
1. **EXTRAER y PRESERVAR toda informaci√≥n proporcionada** - nombres completos con apellidos, cursos, fechas, etc.
2. **CONTAR correctamente los involucrados** - Si hay UN estudiante, habla en SINGULAR. Si hay varios, en PLURAL.
3. **Inferir del contexto** lo m√°ximo posible (ej: "1¬∞ Medio" ‚Üí adolescente ~14-15 a√±os)
4. **Reconocer el nivel de gravedad** del incidente

üõë REGLA CR√çTICA - NO PREGUNTES POR DATOS YA PROPORCIONADOS:
- Si el usuario menciona "Nombre Apellido" ‚Üí USA el nombre completo (NO solo el nombre de pila)
- Si el usuario dice "N¬∞ b√°sico" o "N¬∞ medio" ‚Üí YA TIENES el curso (NO preguntes de nuevo)
- Si el usuario da nombres con apellidos ‚Üí CONSERVA los apellidos completos en tu respuesta
- ANTES de preguntar, verifica si el dato ya est√° en el mensaje del usuario

üìä SINGULAR VS PLURAL - ADAPTA TU LENGUAJE:
- Si falta informaci√≥n cr√≠tica (tipo, fecha, descripci√≥n completa), p√≠dela amablemente pero con formalidad.

SOBRE INFORMACI√ìN SENSIBLE (TEA/NEE):
- NO preguntes por diagn√≥sticos TEA/NEE de forma rutinaria u obligatoria.
- SOLAMENTE pregunta si:
  a) Es estrictamente necesario para el contexto del incidente.
  b) El usuario ha mencionado dificultades de aprendizaje o comportamiento.
  c) La normativa o protocolo espec√≠fico lo exige expl√≠citamente para el tipo de caso.

**PASO 2 - RESPONDER + RECOPILAR:**
Si el usuario pregunt√≥ algo espec√≠fico:
1. RESPONDE la pregunta con orientaci√≥n clara y profesional.
2. Luego, solicita los datos faltantes (si los hay).

Si NO pregunt√≥ nada espec√≠fico:
1. Reconoce la situaci√≥n con seriedad profesional.
2. Agrupa los datos faltantes en UNA pregunta concisa.

**PASO 3 - CONVERGENCIA:**
- Una vez tengas la informaci√≥n suficiente, confirma los datos.
- Ofrece proceder con la gesti√≥n del caso.

TONO DE VOZ:
- **Formal y Profesional (Espa√±ol Neutro):** Mant√©n una distancia profesional pero colaborativa. Nada de modismos chilenos o coloquialismos.
- **Directo pero cort√©s:** Evita saludos tipo carta ("Estimado usuario"), habla directamente como un asistente experto.
- **NO uses:** "Estimado", "Cordialmente", ni despedidas de correo en el chat.
- **Ejemplo correcto:** "Entiendo la situaci√≥n. Para proceder adecuadamente con el registro, necesito que me indique el curso del estudiante involucrado."
- **Ejemplo incorrecto:** "Hola compadre, cu√©ntame qu√© pas√≥." / "Estimado Director, le informo que..."

NO HAGAS:
- Inventar documentos.
- Activar protocolos autom√°ticamente.
- Dar consejos cl√≠nicos.
- Redactar correos en el chat (usa la herramienta de redactar correo).

Si el usuario necesita enviar correos, ind√≠cale que puedes ayudarle a redactarlos usando la funci√≥n del sistema."""

        # Agregar contexto del usuario si existe
        if user_context:
            user_info = f"""

INFORMACI√ìN DEL USUARIO:
- Nombre: {user_context.get('nombre', 'No especificado')}
- Rol: {user_context.get('rol', 'No especificado')}

Puedes personalizar tu respuesta seg√∫n su rol."""
            base_prompt += user_info
        
        # Add RAG context section
        if rag_context and rag_context.get("rice_formatted"):
            rice_section = f"""

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
‚úÖ CONTEXTO RAG - FRAGMENTOS RELEVANTES DEL RICE Y MARCO LEGAL
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

{rag_context['rice_formatted']}

INSTRUCCIONES CR√çTICAS:
- BASA tu respuesta en los fragmentos espec√≠ficos mostrados arriba
- CITA art√≠culos/secciones cuando los menciones
- SIGUE los protocolos descritos en los fragmentos
- NO inventes informaci√≥n que no est√© en los fragmentos

‚ö†Ô∏è IMPORTANTE - MANEJO DE INFORMACI√ìN FALTANTE:
- Si buscas un protocolo espec√≠fico y NO aparece en los fragmentos anteriores, di:
  "Este protocolo espec√≠fico no est√° presente en los fragmentos del RICE consultados"
- NUNCA digas "no tengo acceso" o "no tengo el contexto completo"
- El documento RICE YA EST√Å CARGADO - si algo no aparece, es porque no est√° en el documento
- Si no encuentras un protocolo espec√≠fico pero hay procedimientos generales aplicables, √∫salos

Tokens usados: ~{rag_context.get('total_tokens', 0)} tokens
"""
        else:
            rice_section = f"""

‚ö†Ô∏è RICE NO DISPONIBLE - MODO GEN√âRICO
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

NO se encontraron fragmentos espec√≠ficos del RICE.

INSTRUCCIONES:
- Proporciona orientaci√≥n GENERAL seg√∫n normativa chilena est√°ndar
- INDICA claramente que son recomendaciones generales
- RECOMIENDA verificar con el RICE espec√≠fico del {school_name}

üí° Nota: El RICE debe estar indexado en Vertex AI Search para b√∫squeda RAG."""

        base_prompt += rice_section
        
        # NUEVO: Inyectar datos ya conocidos si existen
        if known_data:
            known_data_formatted = self._format_known_data(known_data)
            known_section = f"""

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
üõë DATOS YA PROPORCIONADOS POR EL USUARIO - NO VOLVER A PREGUNTAR
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

{known_data_formatted}

‚ö†Ô∏è INSTRUCCI√ìN CR√çTICA: Los datos anteriores YA fueron proporcionados.
NO preguntes nuevamente por ellos. Solo pregunta por datos que FALTAN.
USA los nombres y cursos EXACTOS proporcionados arriba.
"""
            base_prompt += known_section
        
        return base_prompt
    
    async def _classify_case_quick(self, message: str, history: List) -> dict:
        """
        Clasifica r√°pidamente el tipo y gravedad del caso.
        
        Returns:
            {
                "type": "agresi√≥n_f√≠sica" | "bullying" | "conflicto" | etc.,
                "severity": "leve" | "grave" | "grav√≠simo"
            }
        """
        try:
            from langchain_core.messages import HumanMessage, SystemMessage
            
            # Prompt r√°pido para clasificaci√≥n
            classification_prompt = f"""Clasifica este caso escolar BREVEMENTE:

Caso: "{message}"

Responde SOLO con JSON:
{{
  "type": "tipo_de_caso",
  "severity": "leve" | "grave" | "grav√≠simo"
}}

Tipos v√°lidos: agresi√≥n_f√≠sica, bullying, acoso, conflicto, maltrato, discriminaci√≥n, violencia_verbal, otro

Criterios de gravedad:
- leve: Conflictos menores, desacuerdos, falta de respeto ocasional
- grave: Agresiones leves, bullying sostenido, discriminaci√≥n
- grav√≠simo: Agresiones f√≠sicas con lesiones, violencia sexual, amenazas graves"""
            
            messages = [
                SystemMessage(content="Eres un clasificador de casos escolares. Responde solo JSON."),
                HumanMessage(content=classification_prompt)
            ]
            
            response = await self.llm.ainvoke(messages)
            content = response.content.strip()
            
            # Limpiar markdown si existe
            if "```" in content:
                import re
                match = re.search(r'```(?:json)?\s*(\{[\s\S]*?\})\s*```', content, re.DOTALL)
                if match:
                    content = match.group(1)
            
            import json
            classification = json.loads(content)
            
            # Validar y normalizar
            case_type = classification.get("type", "otro")
            severity = classification.get("severity", "leve")
            
            return {
                "type": case_type,
                "severity": severity
            }
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [CASE_CREATION] Error classifying case: {e}")
            # Fallback seguro
            return {
                "type": "otro",
                "severity": "leve"
            }
    
    async def _search_rice_rag(
        self,
        message: str,
        search_app_id: str,
        case_type: str = None,
        severity: str = None
    ) -> dict:
        """
        Busca en RICE y marco legal usando RAG.
        
        Returns:
            {
                "rice_formatted": str,  # Fragmentos del RICE formateados
                "legal_formatted": str, # Fragmentos del marco legal (si aplica)
                "total_tokens": int
            }
        """
        try:
            from app.services.chat.rice_search_service import rice_search_service
            
            # Fallback: usar app demo si no se provee ID
            if not search_app_id:
                search_app_id = "demostracion_1767713503741"
                logger.warning(f"‚ö†Ô∏è [CASE_CREATION] No search_app_id provided, using default: {search_app_id}")
            
            # E jecutar b√∫squeda RAG
            results = await rice_search_service.search_rice_for_case(
                query=message,
                school_search_app_id=search_app_id,
                case_type=case_type,
                severity=severity
            )
            
            # Formatear resultados
            rice_formatted = rice_search_service.format_results_for_prompt(
                rice_results=results.get("rice_results", []),
                legal_results=results.get("legal_results", [])
            )
            
            return {
                "rice_formatted": rice_formatted,
                "total_tokens": results.get("total_tokens", 0)
            }
            
        except Exception as e:
            logger.error(f"‚ùå [CASE_CREATION] Error in RAG search: {e}")
            return {
                "rice_formatted": "",
                "total_tokens": 0
            }
    
    async def _extract_known_data(self, history: List, current_message: str) -> dict:
        """
        Extrae datos ya proporcionados del historial usando un prompt r√°pido.
        
        IMPORTANTE: Esto evita que el LLM re-pregunte informaci√≥n que el usuario ya dio.
        
        Returns:
            {
                "estudiantes": ["Nombre Apellido (N¬∞ b√°sico/medio)"],
                "fecha_incidente": "DD/MM/YYYY",
                "lugar": "Lugar del incidente",
                "tipo_caso": "Tipo de caso",
                "involucrados": ["Relaci√≥n con el estudiante"],
                "descripcion_conducta": "Descripci√≥n breve",
                "tiene_tea_nee": "S√≠/No/No especificado"
            }
        """
        try:
            if not history or len(history) == 0:
                return {}
            
            # Extraer solo mensajes del usuario del historial
            user_messages = []
            for msg in history:
                if isinstance(msg, HumanMessage):
                    content = msg.content if isinstance(msg.content, str) else str(msg.content)
                    user_messages.append(content)
            
            if not user_messages:
                return {}
            
            # Agregar mensaje actual
            all_user_input = "\n---\n".join(user_messages + [current_message])
            
            # Prompt de extracci√≥n muy corto y estructurado
            extraction_prompt = f"""Extrae los datos del caso de los siguientes mensajes del usuario.
SOLO extrae datos que est√©n EXPL√çCITAMENTE mencionados. Si un dato no est√°, pon "No especificado".

MENSAJES:
{all_user_input}

Responde SOLO con JSON (sin markdown):
{{"estudiantes": ["lista de estudiantes con nombre y curso si se mencionan"],
"fecha_incidente": "fecha si se menciona",
"lugar": "lugar si se menciona",
"tipo_caso": "tipo de situaci√≥n",
"involucrados": ["otras personas mencionadas y su relaci√≥n"],
"descripcion_conducta": "breve descripci√≥n de lo ocurrido",
"tiene_tea_nee": "S√≠/No/No especificado"}}"""
            
            messages = [
                SystemMessage(content="Eres un extractor de datos. Responde solo JSON v√°lido."),
                HumanMessage(content=extraction_prompt)
            ]
            
            response = await self.llm.ainvoke(messages)
            content = response.content.strip()
            
            # Limpiar markdown si existe
            if "```" in content:
                import re
                match = re.search(r'```(?:json)?\s*(\{[\s\S]*?\})\s*```', content, re.DOTALL)
                if match:
                    content = match.group(1)
            
            import json
            known_data = json.loads(content)
            
            # Filtrar campos vac√≠os o "No especificado"
            filtered = {}
            for key, value in known_data.items():
                if value and value != "No especificado":
                    if isinstance(value, list):
                        # Filtrar listas vac√≠as o con solo "No especificado"
                        clean_list = [v for v in value if v and v != "No especificado"]
                        if clean_list:
                            filtered[key] = clean_list
                    else:
                        filtered[key] = value
            
            logger.info(f"üìù [CASE_CREATION] Extracted known data: {list(filtered.keys())}")
            return filtered
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [CASE_CREATION] Error extracting known data (non-critical): {e}")
            return {}
    
    def _format_known_data(self, known_data: dict) -> str:
        """
        Formatea los datos conocidos para inyectar en el prompt.
        
        Returns:
            String formateado con los datos ya proporcionados
        """
        if not known_data:
            return "No hay datos previos del caso."
        
        lines = []
        
        field_labels = {
            "estudiantes": "üë§ Estudiante(s)",
            "fecha_incidente": "üìÖ Fecha",
            "lugar": "üìç Lugar",
            "tipo_caso": "üè∑Ô∏è Tipo de caso",
            "involucrados": "üë• Otros involucrados",
            "descripcion_conducta": "üìù Descripci√≥n",
            "tiene_tea_nee": "üß† TEA/NEE"
        }
        
        for key, label in field_labels.items():
            value = known_data.get(key)
            if value:
                if isinstance(value, list):
                    # Asegurarse de que todos los items sean strings
                    str_items = [str(item) if not isinstance(item, str) else item for item in value]
                    lines.append(f"{label}: {', '.join(str_items)}")
                elif isinstance(value, dict):
                    # Si es un dict, convertir a string representativo
                    lines.append(f"{label}: {str(value)}")
                else:
                    lines.append(f"{label}: {value}")
        
        return "\n".join(lines) if lines else "No hay datos previos del caso."

    def _sanitize_messages(self, messages: List) -> List:
        """
        Filtra archivos adjuntos demasiado grandes (>20MB) de los mensajes
        para evitar errores de Vertex AI (400 InvalidArgument).
        """
        try:
            clean_messages = []
            for msg in messages:
                # Si el contenido es string, pasa directo
                if isinstance(msg.content, str):
                    clean_messages.append(msg)
                    continue
                
                # Si es lista (multimodal)
                if isinstance(msg.content, list):
                    new_content = []
                    has_changes = False
                    
                    for part in msg.content:
                        # Identificar partes de archivo (file_uri o media)
                        file_uri = None
                        if isinstance(part, dict):
                            file_uri = part.get('file_uri') or part.get('image_url')
                        
                        if file_uri and isinstance(file_uri, str) and file_uri.startswith("gs://"):
                            # Verificar tama√±o
                            try:
                                parts = file_uri.replace("gs://", "").split("/", 1)
                                if len(parts) == 2:
                                    bucket_name, blob_path = parts
                                    bucket = storage_service.client.bucket(bucket_name)
                                    blob = bucket.blob(blob_path)
                                    blob.reload()
                                    
                                    if blob.size and blob.size > 20 * 1024 * 1024: # 20MB limit
                                        logger.warning(f"‚ö†Ô∏è [CASE_CREATION] Dropping large file from history: {blob.name} ({blob.size} bytes)")
                                        # Reemplazar con texto placeholder
                                        new_content.append({
                                            "type": "text", 
                                            "text": f"[Archivo adjunto omitido por tama√±o excesivo: {blob.name}]"
                                        })
                                        has_changes = True
                                        continue
                            except Exception as e:
                                logger.error(f"Error checking file size in sanitizer: {e}")
                        
                        # Si no es grande o fall√≥ check, mantener
                        new_content.append(part)
                    
                    # Reconstruir mensaje con contenido limpio
                    if has_changes:
                        from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
                        if isinstance(msg, HumanMessage):
                            clean_messages.append(HumanMessage(content=new_content))
                        elif isinstance(msg, SystemMessage):
                            clean_messages.append(SystemMessage(content=new_content))
                        elif isinstance(msg, AIMessage):
                            clean_messages.append(AIMessage(content=new_content))
                        else:
                            clean_messages.append(msg) # Tipo desconocido, pasar igual
                    else:
                        clean_messages.append(msg)
                else:
                    clean_messages.append(msg)
            
            return clean_messages
            
        except Exception as e:
            logger.error(f"‚ùå [CASE_CREATION] Error sanitizing messages: {e}")
            return messages # Fallback: return originals


# Singleton instance
case_creation_service = CaseCreationService()
